{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A function used to build a vocabulary based on descending word frequencies \n",
    "# def build_vocab(sentences):\n",
    "#     # Build vocabulary\n",
    "#     word_counts = Counter(itertools.chain(*sentences))\n",
    "#     # Mapping from index to word\n",
    "#     vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "#     # Mapping from word to index\n",
    "#     vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "#     return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A function used to learn word embeddings through Word2vec module\n",
    "# def get_embeddings(inp_data, vocabulary_inv, size_features=100,\n",
    "#                    mode='skipgram',\n",
    "#                    min_word_count=2,\n",
    "#                    context=5):\n",
    "#     model_name = \"embedding\"\n",
    "#     model_name = os.path.join(model_name)\n",
    "#     num_workers = 15  # Number of threads to run in parallel\n",
    "#     downsampling = 1e-3  # Downsample setting for frequent words\n",
    "#     print('Training Word2Vec model...')\n",
    "#     # use inp_data and vocabulary_inv to reconstruct sentences\n",
    "#     sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "#     if mode == 'skipgram':\n",
    "#         sg = 1\n",
    "#         print('Model: skip-gram')\n",
    "#     elif mode == 'cbow':\n",
    "#         sg = 0\n",
    "#         print('Model: CBOW')\n",
    "#     embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "#                                         sg=sg,\n",
    "#                                         size=size_features,\n",
    "#                                         min_count=min_word_count,\n",
    "#                                         window=context,\n",
    "#                                         sample=downsampling)\n",
    "#     embedding_model.init_sims(replace=True)\n",
    "#     print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "#     embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "#     for i in range(len(vocabulary_inv)):\n",
    "#         word = vocabulary_inv[i]\n",
    "#         if word in embedding_model:\n",
    "#             embedding_weights[i] = embedding_model[word]\n",
    "#         else:\n",
    "#             embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
    "#                                                      embedding_model.vector_size)\n",
    "#     return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    \n",
    "    # get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('would')\n",
    "    # prepare translation table to translate punctuation to space\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    preprocessed_sentences = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = row[\"text\"]\n",
    "        sent_nopuncts = sent.translate(translator)\n",
    "        words_list = sent_nopuncts.strip().split()\n",
    "        # 将每个词转化为小写形式\n",
    "        filtered_words = [word.lower() for word in words_list if word.lower() not in stop_words and len(word) != 1] # also skip space from above translation\n",
    "\n",
    "#         filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1] # also skip space from above translation\n",
    "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
    "    df[\"text\"] = preprocessed_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../\"\n",
    "\n",
    "df_train = pd.read_csv(data_path + \"train.csv\")\n",
    "df_test = pd.read_csv(data_path + \"test.csv\")\n",
    "\n",
    "df_train[\"text\"] = df_train[\"review\"]\n",
    "df_test[\"text\"] = df_test[\"review\"]\n",
    "df_train = preprocess_df(df_train)\n",
    "df_test = preprocess_df(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # tokenization \n",
    "# tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "# # build vocabulary from tokenized data\n",
    "# word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
    "# # use the above mapping to create input data\n",
    "# inp_data = [[vocabulary[word] for word in text] for text in tagged_data]\n",
    "# # get embedding vector\n",
    "# embedding_weights = get_embeddings(inp_data, vocabulary_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(df_test[\"text\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 引入word2vec模型\n",
    "# import gensim.downloader as api\n",
    "# embedding_weights = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词语级tf-idf  矩阵代表了每个词语在不同文档中的TF-IDF\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(df_train['text'])\n",
    "train_vec = tfidf_vect.transform(df_train['text'])\n",
    "test_vec = tfidf_vect.transform(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df_train[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13144"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13144, 5000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset and dev dataset\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(train_vec, label, test_size=0.2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# 定义SMOTE模型，random_state相当于随机数种子的作用\n",
    "smo = SMOTE(random_state=42)\n",
    "X_train, y_train = smo.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "        american (new)       0.46      0.48      0.47       271\n",
      "american (traditional)       0.69      0.73      0.71       545\n",
      "          asian fusion       0.42      0.44      0.43        61\n",
      "        canadian (new)       0.37      0.62      0.46        78\n",
      "               chinese       0.94      0.93      0.93       338\n",
      "               italian       0.93      0.83      0.88       412\n",
      "              japanese       0.92      0.86      0.89       226\n",
      "         mediterranean       0.90      0.78      0.84       137\n",
      "               mexican       0.98      0.94      0.96       460\n",
      "                  thai       0.96      0.91      0.93       101\n",
      "\n",
      "              accuracy                           0.79      2629\n",
      "             macro avg       0.76      0.75      0.75      2629\n",
      "          weighted avg       0.81      0.79      0.80      2629\n",
      "\n",
      "acc:0.7945987067325979, f1:0.7508002268796533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# logistic classfication model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_dev)\n",
    "print(classification_report(y_dev, prediction))\n",
    "\n",
    "acc = metrics.accuracy_score(y_dev, prediction)\n",
    "f1 = metrics.f1_score(y_dev, prediction,average='macro')\n",
    "print(f'acc:{acc}, f1:{f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "        american (new)       0.40      0.18      0.25       271\n",
      "american (traditional)       0.61      0.83      0.70       545\n",
      "          asian fusion       0.75      0.10      0.17        61\n",
      "        canadian (new)       0.46      0.17      0.25        78\n",
      "               chinese       0.87      0.95      0.91       338\n",
      "               italian       0.85      0.89      0.87       412\n",
      "              japanese       0.91      0.93      0.92       226\n",
      "         mediterranean       0.87      0.80      0.83       137\n",
      "               mexican       0.90      0.94      0.92       460\n",
      "                  thai       0.92      0.89      0.90       101\n",
      "\n",
      "              accuracy                           0.78      2629\n",
      "             macro avg       0.76      0.67      0.67      2629\n",
      "          weighted avg       0.76      0.78      0.75      2629\n",
      "\n",
      "acc:0.7797641688855078, f1:0.673095839388868\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_dev)\n",
    "print(classification_report(y_dev, prediction))\n",
    "\n",
    "acc = metrics.accuracy_score(y_dev, prediction)\n",
    "f1 = metrics.f1_score(y_dev, prediction,average='macro')\n",
    "print(f'acc:{acc}, f1:{f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "        american (new)       0.51      0.34      0.41       271\n",
      "american (traditional)       0.62      0.91      0.73       545\n",
      "          asian fusion       0.52      0.26      0.35        61\n",
      "        canadian (new)       0.45      0.26      0.33        78\n",
      "               chinese       0.89      0.94      0.91       338\n",
      "               italian       0.94      0.84      0.89       412\n",
      "              japanese       0.92      0.88      0.90       226\n",
      "         mediterranean       0.92      0.74      0.82       137\n",
      "               mexican       0.98      0.92      0.95       460\n",
      "                  thai       0.97      0.86      0.91       101\n",
      "\n",
      "              accuracy                           0.80      2629\n",
      "             macro avg       0.77      0.70      0.72      2629\n",
      "          weighted avg       0.80      0.80      0.79      2629\n",
      "\n",
      "acc:0.797641688855078, f1:0.720334977679156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_dev)\n",
    "print(classification_report(y_dev, prediction))\n",
    "\n",
    "acc = metrics.accuracy_score(y_dev, prediction)\n",
    "f1 = metrics.f1_score(y_dev, prediction,average='macro')\n",
    "print(f'acc:{acc}, f1:{f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "        american (new)       0.22      0.23      0.22       271\n",
      "american (traditional)       0.53      0.53      0.53       545\n",
      "          asian fusion       0.11      0.18      0.14        61\n",
      "        canadian (new)       0.27      0.38      0.32        78\n",
      "               chinese       0.80      0.77      0.79       338\n",
      "               italian       0.79      0.73      0.76       412\n",
      "              japanese       0.83      0.75      0.79       226\n",
      "         mediterranean       0.64      0.57      0.60       137\n",
      "               mexican       0.86      0.85      0.85       460\n",
      "                  thai       0.84      0.89      0.87       101\n",
      "\n",
      "              accuracy                           0.64      2629\n",
      "             macro avg       0.59      0.59      0.59      2629\n",
      "          weighted avg       0.66      0.64      0.65      2629\n",
      "\n",
      "acc:0.6397869912514264, f1:0.5864400283875524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_dev)\n",
    "print(classification_report(y_dev, prediction))\n",
    "\n",
    "acc = metrics.accuracy_score(y_dev, prediction)\n",
    "f1 = metrics.f1_score(y_dev, prediction,average='macro')\n",
    "print(f'acc:{acc}, f1:{f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "        american (new)       0.30      0.59      0.40       271\n",
      "american (traditional)       0.92      0.04      0.08       545\n",
      "          asian fusion       0.16      0.49      0.24        61\n",
      "        canadian (new)       0.12      0.62      0.20        78\n",
      "               chinese       0.93      0.82      0.87       338\n",
      "               italian       0.92      0.72      0.81       412\n",
      "              japanese       0.85      0.83      0.84       226\n",
      "         mediterranean       0.71      0.76      0.73       137\n",
      "               mexican       0.98      0.82      0.89       460\n",
      "                  thai       0.88      0.93      0.90       101\n",
      "\n",
      "              accuracy                           0.61      2629\n",
      "             macro avg       0.68      0.66      0.60      2629\n",
      "          weighted avg       0.81      0.61      0.61      2629\n",
      "\n",
      "acc:0.608596424496006, f1:0.5976158130873548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "prediction = model.predict(X_dev)\n",
    "print(classification_report(y_dev, prediction))\n",
    "\n",
    "acc = metrics.accuracy_score(y_dev, prediction)\n",
    "f1 = metrics.f1_score(y_dev, prediction,average='macro')\n",
    "print(f'acc:{acc}, f1:{f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对整体的训练集进行smote数据增强\n",
    "# 定义SMOTE模型，random_state相当于随机数种子的作用\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE()\n",
    "train_vec, label = oversample.fit_resample(train_vec, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "        american (new)       0.98      0.95      0.96       271\n",
      "american (traditional)       0.97      0.98      0.98       545\n",
      "          asian fusion       1.00      1.00      1.00        61\n",
      "        canadian (new)       0.95      1.00      0.97        78\n",
      "               chinese       1.00      1.00      1.00       338\n",
      "               italian       1.00      0.99      0.99       412\n",
      "              japanese       0.99      1.00      1.00       226\n",
      "         mediterranean       0.98      0.99      0.99       137\n",
      "               mexican       1.00      0.99      0.99       460\n",
      "                  thai       1.00      1.00      1.00       101\n",
      "\n",
      "              accuracy                           0.99      2629\n",
      "             macro avg       0.99      0.99      0.99      2629\n",
      "          weighted avg       0.99      0.99      0.99      2629\n",
      "\n",
      "acc:0.9874476987447699, f1:0.9882910874110443\n"
     ]
    }
   ],
   "source": [
    "# 通过上面模型的观察，发现在验证集上效果最好的是SVC模型，所以在这里使用所有数据训练SVC模型\n",
    "# 注意，train_vec表示所有数据，这里我依然是用X_dev和y_dev进行验证，其实这些数据已经被包含在train_vec，label里了\n",
    "# 但是没关系，这里只是最后看一眼效果，并不是真的再拿它当验证集\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(train_vec, label)\n",
    "prediction = model.predict(X_dev)\n",
    "print(classification_report(y_dev, prediction))\n",
    "\n",
    "acc = metrics.accuracy_score(y_dev, prediction)\n",
    "f1 = metrics.f1_score(y_dev, prediction,average='macro')\n",
    "print(f'acc:{acc}, f1:{f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_vec)\n",
    "# in your implemetation, create the output file using the same format\n",
    "dic = {\"Id\": [], \"Predicted\": []}\n",
    "for i, pred in enumerate(preds):\n",
    "    dic[\"Id\"].append(i)\n",
    "    dic[\"Predicted\"].append(pred)\n",
    "\n",
    "dic_df = pd.DataFrame.from_dict(dic)\n",
    "dic_df.to_csv(data_path + \"predicted-tfidf-SVM-smote.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
